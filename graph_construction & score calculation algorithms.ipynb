{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f80615b-8e6f-4924-9105-ca92cd27056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import inflect\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import json\n",
    "\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc967671-f2ae-4cb2-af3d-92e898051d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(file_name):\n",
    "    return pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b930ee89-814c-4a4c-a66e-5b8440b9e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_noun(word):\n",
    "    tokenized_word = word_tokenize(word)\n",
    "    tagged_word = pos_tag(tokenized_word)\n",
    "    if len(tagged_word) >= 1:\n",
    "        return tagged_word[0][1] in ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33385ab1-7f90-46e1-9656-54973d6cc68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning_process(df, with_loss_amount = True):\n",
    "    inputs = df[\"inputs\"].tolist()\n",
    "    outputs = df[\"outputs\"].tolist()\n",
    "    loss = df[\"lossAmount\"].tolist()\n",
    "    indices_to_remove_for_empty = []\n",
    "    for i in range(len(inputs)):\n",
    "        if type(inputs[i]) == float:\n",
    "            indices_to_remove_for_empty.append(i)\n",
    "    for i in range(len(outputs)):\n",
    "        if type(outputs[i]) == float:\n",
    "            indices_to_remove_for_empty.append(i)\n",
    "    if with_loss_amount == True:\n",
    "        loss = [str(number) for number in loss]\n",
    "        for i in range(len(loss)):\n",
    "            if loss[i] == \"nan\":\n",
    "                indices_to_remove_for_empty.append(i)\n",
    "    indices_to_remove_for_empty = list(set(indices_to_remove_for_empty))\n",
    "    \n",
    "    df = df.drop(index = indices_to_remove_for_empty)\n",
    "    inputs = df[\"inputs\"].tolist()\n",
    "    outputs = df[\"outputs\"].tolist()\n",
    "    loss = df[\"lossAmount\"].tolist()\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        indices_to_remove = []\n",
    "        for j in range(len(inp)):\n",
    "            if len(inp[j]) == 1:\n",
    "                indices_to_remove.append(j)\n",
    "        inp = [k for m, k in enumerate(inp) if m not in indices_to_remove]\n",
    "        inputs[i] = \",\".join(inp)\n",
    "    for i in range(len(outputs)):\n",
    "        outp = outputs[i].split(\",\")\n",
    "        indices_to_remove = []\n",
    "        for j in range(len(outp)):\n",
    "            if len(outp[j]) == 1:\n",
    "                indices_to_remove.append(j)\n",
    "        outp = [k for m, k in enumerate(outp) if m not in indices_to_remove]\n",
    "        outputs[i] = \",\".join(outp)\n",
    "\n",
    "    inputs = [s.lower() for s in inputs]\n",
    "    outputs = [s.lower() for s in outputs]\n",
    "    inputs = [re.sub(r\"\\(.*?\\)\", \"\", s) for s in inputs]\n",
    "    outputs = [re.sub(r\"\\(.*?\\)\", \"\", s) for s in outputs]\n",
    "    inputs = [re.sub(r\"'s|'\", \"\", s) for s in inputs]\n",
    "    outputs = [re.sub(r\"'s|'\", \"\", s) for s in outputs]\n",
    "\n",
    "    # noun or not\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            ns = inp[j].split(\" \")\n",
    "            indices = []\n",
    "            for k in range(len(ns)):\n",
    "                if is_noun(ns[k]) == False:\n",
    "                    if k != 0 and k!= len(ns) - 1:\n",
    "                        if is_noun(ns[k-1]) == False or is_noun(ns[k+1]) == False:\n",
    "                            indices.append(k)\n",
    "                    elif k == 0:\n",
    "                        try:\n",
    "                            if is_noun(ns[k+1]) == False:\n",
    "                                indices.append(k)\n",
    "                        except:\n",
    "                            print(ns[k])\n",
    "                    else:\n",
    "                        indices.append(k)\n",
    "            ns = [l for m, l in enumerate(ns) if m not in indices]\n",
    "            inp[j] = \" \".join(ns)\n",
    "            if inp[j] == \"medical diagnosis\":\n",
    "                inp[j] = \"diagnosis\"\n",
    "            if inp[j] == \"voter identfication card\":\n",
    "                inp[j] = \"voter identification card\"\n",
    "        inputs[i] = \",\".join(inp)\n",
    "        \n",
    "    \n",
    "    for i in range(len(outputs)):\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(outp)):\n",
    "            ns = outp[j].split(\" \")\n",
    "            indices = []\n",
    "            for k in range(len(ns)):\n",
    "                if is_noun(ns[k]) == False:\n",
    "                    if k != 0 and k!= len(ns) - 1:\n",
    "                        if is_noun(ns[k-1]) == False or is_noun(ns[k+1]) == False:\n",
    "                            indices.append(k)\n",
    "                    elif k == 0:\n",
    "                        try:\n",
    "                            if is_noun(ns[k+1]) == False:\n",
    "                                indices.append(k)\n",
    "                        except:\n",
    "                            print(ns[k])\n",
    "                    else:\n",
    "                        indices.append(k)\n",
    "            ns = [l for m, l in enumerate(ns) if m not in indices]\n",
    "            outp[j] = \" \".join(ns)\n",
    "            if outp[j] == \"medical diagnosis\":\n",
    "                outp[j] = \"diagnosis\"\n",
    "            if outp[j] == \"voter identfication card\":\n",
    "                outp[j] = \"voter identification card\"\n",
    "        outputs[i] = \",\".join(outp)\n",
    "\n",
    "    #plural to singular\n",
    "    p = inflect.engine()\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            ns = inp[j].split(\" \")\n",
    "            indices = []\n",
    "            for k in range(len(ns)):\n",
    "                if ns[k] != \"\":\n",
    "                    if p.singular_noun(ns[k]):\n",
    "                        if len(wn.synsets(p.singular_noun(ns[k]))) > 0 and ns[k] != \"data\":\n",
    "                            ns[k] = p.singular_noun(ns[k])\n",
    "                else:\n",
    "                    indices.append(k)\n",
    "            ns = [l for m, l in enumerate(ns) if m not in indices]\n",
    "            inp[j] = \" \".join(ns)\n",
    "        inputs[i] = \",\".join(inp)\n",
    "    \n",
    "    for i in range(len(outputs)):\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(outp)):\n",
    "            ns = outp[j].split(\" \")\n",
    "            indices = []\n",
    "            for k in range(len(ns)):\n",
    "                if ns[k] != \"\":\n",
    "                    if p.singular_noun(ns[k]):\n",
    "                        if len(wn.synsets(p.singular_noun(ns[k]))) > 0 and ns[k] != \"data\":\n",
    "                            ns[k] = p.singular_noun(ns[k])\n",
    "                else:\n",
    "                    indices.append(k)\n",
    "            ns = [l for m, l in enumerate(ns) if m not in indices]\n",
    "            outp[j] = \" \".join(ns)\n",
    "        outputs[i] = \",\".join(outp)\n",
    "        \n",
    "    if with_loss_amount == False:\n",
    "        return inputs, outputs\n",
    "    else:\n",
    "        return inputs, outputs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63bca902-9072-4fc4-80f8-e7ca6904c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_construction_return_loss_n_frequency(inputs, outputs, loss):\n",
    "    graph = nx.DiGraph()\n",
    "    weight_dict = {}\n",
    "    node_loss_dict = {}\n",
    "    node_freq_dict = {}\n",
    "    node_avg_loss_dict = {}\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            for k in range(len(outp)):\n",
    "                weight_dict[(inp[j], outp[k])] = 0\n",
    "                node_loss_dict[inp[j]] = 0\n",
    "                node_freq_dict[inp[j]] = 0\n",
    "                node_avg_loss_dict[inp[j]] = 0\n",
    "                node_loss_dict[outp[k]] = 0\n",
    "                node_freq_dict[outp[k]] = 0\n",
    "                node_avg_loss_dict[outp[k]] = 0\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            for k in range(len(outp)):\n",
    "                weight_dict[(inp[j], outp[k])] += 1\n",
    "                if loss[i] != 1 and loss[i] != 0:\n",
    "                    node_loss_dict[inp[j]] += loss[i] \n",
    "                    node_freq_dict[inp[j]] += 1\n",
    "                    node_avg_loss_dict[inp[j]] = node_avg_loss_dict[inp[j]]/node_freq_dict[inp[j]]\n",
    "                    node_loss_dict[outp[k]] += loss[i]\n",
    "                    node_freq_dict[outp[k]] += 1\n",
    "                    node_avg_loss_dict[outp[k]] += node_loss_dict[outp[k]]/node_freq_dict[outp[k]]\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            for k in range(len(outp)):\n",
    "                graph.add_edge(inp[j], outp[k], weight = weight_dict[(inp[j], outp[k])])\n",
    "    return graph, node_loss_dict, node_freq_dict, node_avg_loss_dict\n",
    "\n",
    "\n",
    "\n",
    "def graph_construction_using_loss_weight_return_loss_n_frequency(inputs, outputs, loss):\n",
    "    graph = nx.DiGraph()\n",
    "    weight_dict = {}\n",
    "    weight_dict2 = {}\n",
    "    node_loss_dict = {}\n",
    "    node_freq_dict = {}\n",
    "    node_avg_loss_dict = {}\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            for k in range(len(outp)):\n",
    "                weight_dict[(inp[j], outp[k])] = 0\n",
    "                weight_dict2[(inp[j], outp[k])] = 0\n",
    "                node_loss_dict[inp[j]] = 0\n",
    "                node_freq_dict[inp[j]] = 0\n",
    "                node_avg_loss_dict[inp[j]] = 0\n",
    "                node_loss_dict[outp[k]] = 0\n",
    "                node_freq_dict[outp[k]] = 0\n",
    "                node_avg_loss_dict[outp[k]] = 0\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            for k in range(len(outp)):\n",
    "                weight_dict[(inp[j], outp[k])] += 1\n",
    "                weight_dict2[(inp[j], outp[k])] += loss[i]\n",
    "                if loss[i] != 1 and loss[i] != 0:\n",
    "                    node_loss_dict[inp[j]] += loss[i] \n",
    "                    node_freq_dict[inp[j]] += 1\n",
    "                    node_avg_loss_dict[inp[j]] = node_avg_loss_dict[inp[j]]/node_freq_dict[inp[j]]\n",
    "                    node_loss_dict[outp[k]] += loss[i]\n",
    "                    node_freq_dict[outp[k]] += 1\n",
    "                    node_avg_loss_dict[outp[k]] += node_loss_dict[outp[k]]/node_freq_dict[outp[k]]\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            for k in range(len(outp)):\n",
    "                graph.add_edge(inp[j], outp[k], weight = weight_dict2[(inp[j], outp[k])])\n",
    "    return graph, node_loss_dict, node_freq_dict, node_avg_loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e537876-1fbc-4c24-b05d-1ad0161fb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_scores_with_initialization(graph, initialization, max_iter=100):\n",
    "    return nx.pagerank(graph, weight = \"weight\", nstart = initialization)\n",
    "\n",
    "def get_eHITS_score(graph, max_in_degree, max_out_degree, node_loss_dict, node_initialization = False, n_iter = 20):\n",
    "    nodes = list(graph.nodes())\n",
    "    hubs = {node: 1 for node in list(graph.nodes())}\n",
    "    authorities = {node: 1 for node in list(graph.nodes())}\n",
    "    if node_initialization == True:\n",
    "        hubs = {node: node_loss_dict[node] for node in list(graph.nodes())}\n",
    "        authorities = {node: node_loss_dict[node] for node in list(graph.nodes())}\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        for node_i in nodes:\n",
    "            authorities[node_i] = sum(hubs[node_j]*graph[node_j][node_i]['weight']/max_in_degree for node_j in nodes if node_i in list(graph[node_j]))\n",
    "            hubs[node_i] = sum(authorities[node_j]*graph[node_i][node_j]['weight']/max_out_degree for node_j in list(graph[node_i]))\n",
    "        s = sum(hubs[node]**2 for node in nodes)\n",
    "        print(s)\n",
    "        sum_hubs = np.sqrt(s)\n",
    "        sum_authorities = np.sqrt(sum(authorities[node]**2 for node in nodes))\n",
    "        hubs = {node: hubs[node]/sum_hubs if sum_hubs > 0 else 0 for node in nodes}\n",
    "        authorities = {node: authorities[node]/sum_hubs if sum_authorities > 0 else 0 for node in nodes}\n",
    "    return hubs, authorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e95c7d-0de3-444a-a5d4-d7f30a0e419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_n_scale_pagerank_to_with_90_quantile(pr):\n",
    "    max_pr = np.percentile(list(pr.values()), 90)\n",
    "    scores = {}\n",
    "    for key, value in pr.items():\n",
    "        scores[key] = round(value/max_pr*100,2)\n",
    "    return scores\n",
    "\n",
    "def normalize_n_scale_pagerank_to_with_90_quantile_list(l):\n",
    "    max_pr = np.percentile(l, 90)\n",
    "    #print(max_pr)\n",
    "    scores = []\n",
    "    for value in l:\n",
    "        scores.append(round(value/max_pr*100,2))\n",
    "       # print(round(value/max_pr*100,2))\n",
    "    return scores\n",
    "\n",
    "def normalize_n_scale_pagerank_to_with_90_quantile_list_n_whole(l, pr):\n",
    "    max_pr = np.percentile(list(pr.values()), 90)\n",
    "    #print(max_pr)\n",
    "    scores = []\n",
    "    for value in l:\n",
    "        scores.append(round(value/max_pr*100,2))\n",
    "        #print(round(value/max_pr*100,2))\n",
    "    return scores\n",
    "\n",
    "def normalize_n_scale_score(dictionary):\n",
    "    max_pr = np.max(list(dictionary.values()))\n",
    "    scores = {}\n",
    "    for key, value in dictionary.items():\n",
    "        scores[key] = round(value/max_pr*100,2)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b53bbb0e-d7be-4f40-86f4-9e14a3c47605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correlation(dict1, dict2):\n",
    "    l1 = list(dict1.values())\n",
    "    l2 = list(dict2.values())\n",
    "    correlation, p_value = spearmanr(l1, l2)\n",
    "    return correlation, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad1c779d-4a97-4f06-a1a8-53b0addafc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataframe(\"example_dataset.csv\")\n",
    "inputs, outputs, loss = data_cleaning_process(df, with_loss_amount = True)\n",
    "graph, node_loss_dict, node_freq_dict, node_avg_loss_dict = graph_construction_return_loss_n_frequency(inputs, outputs, loss)\n",
    "graph2, node_loss_dict, node_freq_dict, node_avg_loss_dict = graph_construction_using_loss_weight_return_loss_n_frequency(inputs, \n",
    "                                                                                                                          outputs, loss)\n",
    "pr_w_loss_initial = get_pr_scores_with_initialization(graph, node_loss_dict, max_iter=100)\n",
    "pr_w_loss_initial_w_loss_weight = get_pr_scores_with_initialization(graph2, node_loss_dict, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a716ef7-59ad-422d-8d47-6b4ea00ba210",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_in_degree_freq = max(dict(graph.in_degree(weight = 'weight')).values())\n",
    "max_out_degree_freq = max(dict(graph.out_degree(weight = 'weight')).values())\n",
    "\n",
    "max_in_degree_loss = max(dict(graph2.in_degree(weight = 'weight')).values())\n",
    "max_out_degree_loss = max(dict(graph2.out_degree(weight = 'weight')).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28d5a113-9876-4881-a9dc-49f23b25ae44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 20/20 [00:00<00:00, 1512.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4411591220850477\n",
      "0.2923768597529429\n",
      "0.13326967439510795\n",
      "0.13487792701838477\n",
      "0.13508441414154546\n",
      "0.13511030820764733\n",
      "0.13511339698620803\n",
      "0.135113755050591\n",
      "0.13511379589917674\n",
      "0.13511380051646116\n",
      "0.135113801035554\n",
      "0.13511380109372503\n",
      "0.13511380110023147\n",
      "0.13511380110095822\n",
      "0.1351138011010395\n",
      "0.13511380110104854\n",
      "0.1351138011010494\n",
      "0.1351138011010497\n",
      "0.1351138011010496\n",
      "0.1351138011010496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hubs_w_freq, authorities_w_freq = get_eHITS_score(graph, max_in_degree_freq, max_out_degree_freq, node_loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ff26662-1e7a-459f-882c-f61ede60551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 20/20 [00:00<00:00, 1736.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5076085217033446\n",
      "0.32067828940565296\n",
      "0.20208486516441362\n",
      "0.20311234631015876\n",
      "0.20305813880207382\n",
      "0.20305104184912515\n",
      "0.2030504866855615\n",
      "0.20305044892848575\n",
      "0.20305044650453175\n",
      "0.20305044635311884\n",
      "0.2030504463437905\n",
      "0.20305044634321995\n",
      "0.20305044634318511\n",
      "0.20305044634318306\n",
      "0.20305044634318303\n",
      "0.2030504463431829\n",
      "0.20305044634318292\n",
      "0.20305044634318298\n",
      "0.20305044634318298\n",
      "0.20305044634318298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hubs_w_loss, authorities_w_loss = get_eHITS_score(graph2, max_in_degree_loss, max_out_degree_loss, node_loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5519b389-e829-4a1e-9b8f-110f23f82763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 20/20 [00:00<00:00, 1766.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214416811286.41974\n",
      "0.11227205862054696\n",
      "0.13542960801234122\n",
      "0.13517385490191294\n",
      "0.13512216299333404\n",
      "0.13511484578068506\n",
      "0.13511392516224036\n",
      "0.13511381544574133\n",
      "0.13511380273510598\n",
      "0.1351138012855951\n",
      "0.13511380112178645\n",
      "0.13511380110337273\n",
      "0.13511380110130944\n",
      "0.13511380110107876\n",
      "0.13511380110105287\n",
      "0.13511380110104998\n",
      "0.13511380110104954\n",
      "0.1351138011010497\n",
      "0.1351138011010495\n",
      "0.13511380110104976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hubs_w_freq_initLoss, authorities_w_freq_initLoss = get_eHITS_score(graph, max_in_degree_freq, \n",
    "                                                                    max_out_degree_freq, node_loss_dict, node_initialization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91cadfd-0000-4ba6-b060-806d118b3e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
