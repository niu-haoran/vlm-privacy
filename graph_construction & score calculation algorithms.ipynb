{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f80615b-8e6f-4924-9105-ca92cd27056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import inflect\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import json\n",
    "\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc967671-f2ae-4cb2-af3d-92e898051d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(file_name):\n",
    "    return pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b930ee89-814c-4a4c-a66e-5b8440b9e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_noun(word):\n",
    "    tokenized_word = word_tokenize(word)\n",
    "    tagged_word = pos_tag(tokenized_word)\n",
    "    if len(tagged_word) >= 1:\n",
    "        return tagged_word[0][1] in ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33385ab1-7f90-46e1-9656-54973d6cc68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning_process(df, with_loss_amount = True):\n",
    "    inputs = df[\"inputs\"].tolist()\n",
    "    outputs = df[\"outputs\"].tolist()\n",
    "    loss = df[\"lossAmount\"].tolist()\n",
    "    indices_to_remove_for_empty = []\n",
    "    for i in range(len(inputs)):\n",
    "        if type(inputs[i]) == float:\n",
    "            indices_to_remove_for_empty.append(i)\n",
    "    for i in range(len(outputs)):\n",
    "        if type(outputs[i]) == float:\n",
    "            indices_to_remove_for_empty.append(i)\n",
    "    if with_loss_amount == True:\n",
    "        loss = [str(number) for number in loss]\n",
    "        for i in range(len(loss)):\n",
    "            if loss[i] == \"nan\":\n",
    "                indices_to_remove_for_empty.append(i)\n",
    "    indices_to_remove_for_empty = list(set(indices_to_remove_for_empty))\n",
    "    \n",
    "    df = df.drop(index = indices_to_remove_for_empty)\n",
    "    inputs = df[\"inputs\"].tolist()\n",
    "    outputs = df[\"outputs\"].tolist()\n",
    "    loss = df[\"lossAmount\"].tolist()\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        indices_to_remove = []\n",
    "        for j in range(len(inp)):\n",
    "            if len(inp[j]) == 1:\n",
    "                indices_to_remove.append(j)\n",
    "        inp = [k for m, k in enumerate(inp) if m not in indices_to_remove]\n",
    "        inputs[i] = \",\".join(inp)\n",
    "    for i in range(len(outputs)):\n",
    "        outp = outputs[i].split(\",\")\n",
    "        indices_to_remove = []\n",
    "        for j in range(len(outp)):\n",
    "            if len(outp[j]) == 1:\n",
    "                indices_to_remove.append(j)\n",
    "        outp = [k for m, k in enumerate(outp) if m not in indices_to_remove]\n",
    "        outputs[i] = \",\".join(outp)\n",
    "\n",
    "    inputs = [s.lower() for s in inputs]\n",
    "    outputs = [s.lower() for s in outputs]\n",
    "    inputs = [re.sub(r\"\\(.*?\\)\", \"\", s) for s in inputs]\n",
    "    outputs = [re.sub(r\"\\(.*?\\)\", \"\", s) for s in outputs]\n",
    "    inputs = [re.sub(r\"'s|'\", \"\", s) for s in inputs]\n",
    "    outputs = [re.sub(r\"'s|'\", \"\", s) for s in outputs]\n",
    "\n",
    "    # Check if it is noun or not\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            ns = inp[j].split(\" \")\n",
    "            indices = []\n",
    "            for k in range(len(ns)):\n",
    "                if is_noun(ns[k]) == False:\n",
    "                    if k != 0 and k!= len(ns) - 1:\n",
    "                        if is_noun(ns[k-1]) == False or is_noun(ns[k+1]) == False:\n",
    "                            indices.append(k)\n",
    "                    elif k == 0:\n",
    "                        try:\n",
    "                            if is_noun(ns[k+1]) == False:\n",
    "                                indices.append(k)\n",
    "                        except:\n",
    "                            print(ns[k])\n",
    "                    else:\n",
    "                        indices.append(k)\n",
    "            ns = [l for m, l in enumerate(ns) if m not in indices]\n",
    "            inp[j] = \" \".join(ns)\n",
    "            if inp[j] == \"medical diagnosis\":\n",
    "                inp[j] = \"diagnosis\"\n",
    "            if inp[j] == \"voter identfication card\":\n",
    "                inp[j] = \"voter identification card\"\n",
    "        inputs[i] = \",\".join(inp)\n",
    "        \n",
    "    \n",
    "    for i in range(len(outputs)):\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(outp)):\n",
    "            ns = outp[j].split(\" \")\n",
    "            indices = []\n",
    "            for k in range(len(ns)):\n",
    "                if is_noun(ns[k]) == False:\n",
    "                    if k != 0 and k!= len(ns) - 1:\n",
    "                        if is_noun(ns[k-1]) == False or is_noun(ns[k+1]) == False:\n",
    "                            indices.append(k)\n",
    "                    elif k == 0:\n",
    "                        try:\n",
    "                            if is_noun(ns[k+1]) == False:\n",
    "                                indices.append(k)\n",
    "                        except:\n",
    "                            print(ns[k])\n",
    "                    else:\n",
    "                        indices.append(k)\n",
    "            ns = [l for m, l in enumerate(ns) if m not in indices]\n",
    "            outp[j] = \" \".join(ns)\n",
    "            if outp[j] == \"medical diagnosis\":\n",
    "                outp[j] = \"diagnosis\"\n",
    "            if outp[j] == \"voter identfication card\":\n",
    "                outp[j] = \"voter identification card\"\n",
    "        outputs[i] = \",\".join(outp)\n",
    "\n",
    "    # Plural to singular\n",
    "    p = inflect.engine()\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            ns = inp[j].split(\" \")\n",
    "            indices = []\n",
    "            for k in range(len(ns)):\n",
    "                if ns[k] != \"\":\n",
    "                    if p.singular_noun(ns[k]):\n",
    "                        if len(wn.synsets(p.singular_noun(ns[k]))) > 0 and ns[k] != \"data\":\n",
    "                            ns[k] = p.singular_noun(ns[k])\n",
    "                else:\n",
    "                    indices.append(k)\n",
    "            ns = [l for m, l in enumerate(ns) if m not in indices]\n",
    "            inp[j] = \" \".join(ns)\n",
    "        inputs[i] = \",\".join(inp)\n",
    "    \n",
    "    for i in range(len(outputs)):\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(outp)):\n",
    "            ns = outp[j].split(\" \")\n",
    "            indices = []\n",
    "            for k in range(len(ns)):\n",
    "                if ns[k] != \"\":\n",
    "                    if p.singular_noun(ns[k]):\n",
    "                        if len(wn.synsets(p.singular_noun(ns[k]))) > 0 and ns[k] != \"data\":\n",
    "                            ns[k] = p.singular_noun(ns[k])\n",
    "                else:\n",
    "                    indices.append(k)\n",
    "            ns = [l for m, l in enumerate(ns) if m not in indices]\n",
    "            outp[j] = \" \".join(ns)\n",
    "        outputs[i] = \",\".join(outp)\n",
    "        \n",
    "    if with_loss_amount == False:\n",
    "        return inputs, outputs\n",
    "    else:\n",
    "        return inputs, outputs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63bca902-9072-4fc4-80f8-e7ca6904c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_construction_return_loss_n_frequency(inputs, outputs, loss):\n",
    "    graph = nx.DiGraph()\n",
    "    weight_dict = {}\n",
    "    node_loss_dict = {}\n",
    "    node_freq_dict = {}\n",
    "    node_avg_loss_dict = {}\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            for k in range(len(outp)):\n",
    "                weight_dict[(inp[j], outp[k])] = 0\n",
    "                node_loss_dict[inp[j]] = 0\n",
    "                node_freq_dict[inp[j]] = 0\n",
    "                node_avg_loss_dict[inp[j]] = 0\n",
    "                node_loss_dict[outp[k]] = 0\n",
    "                node_freq_dict[outp[k]] = 0\n",
    "                node_avg_loss_dict[outp[k]] = 0\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            for k in range(len(outp)):\n",
    "                weight_dict[(inp[j], outp[k])] += 1\n",
    "                if loss[i] != 1 and loss[i] != 0:\n",
    "                    node_loss_dict[inp[j]] += loss[i] \n",
    "                    node_freq_dict[inp[j]] += 1\n",
    "                    node_avg_loss_dict[inp[j]] = node_avg_loss_dict[inp[j]]/node_freq_dict[inp[j]]\n",
    "                    node_loss_dict[outp[k]] += loss[i]\n",
    "                    node_freq_dict[outp[k]] += 1\n",
    "                    node_avg_loss_dict[outp[k]] += node_loss_dict[outp[k]]/node_freq_dict[outp[k]]\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            for k in range(len(outp)):\n",
    "                graph.add_edge(inp[j], outp[k], weight = weight_dict[(inp[j], outp[k])])\n",
    "    return graph, node_loss_dict, node_freq_dict, node_avg_loss_dict\n",
    "\n",
    "\n",
    "\n",
    "def graph_construction_using_loss_weight_return_loss_n_frequency(inputs, outputs, loss):\n",
    "    graph = nx.DiGraph()\n",
    "    weight_dict = {}\n",
    "    weight_dict2 = {}\n",
    "    node_loss_dict = {}\n",
    "    node_freq_dict = {}\n",
    "    node_avg_loss_dict = {}\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            for k in range(len(outp)):\n",
    "                weight_dict[(inp[j], outp[k])] = 0\n",
    "                weight_dict2[(inp[j], outp[k])] = 0\n",
    "                node_loss_dict[inp[j]] = 0\n",
    "                node_freq_dict[inp[j]] = 0\n",
    "                node_avg_loss_dict[inp[j]] = 0\n",
    "                node_loss_dict[outp[k]] = 0\n",
    "                node_freq_dict[outp[k]] = 0\n",
    "                node_avg_loss_dict[outp[k]] = 0\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            for k in range(len(outp)):\n",
    "                weight_dict[(inp[j], outp[k])] += 1\n",
    "                weight_dict2[(inp[j], outp[k])] += loss[i]\n",
    "                if loss[i] != 1 and loss[i] != 0:\n",
    "                    node_loss_dict[inp[j]] += loss[i] \n",
    "                    node_freq_dict[inp[j]] += 1\n",
    "                    node_avg_loss_dict[inp[j]] = node_avg_loss_dict[inp[j]]/node_freq_dict[inp[j]]\n",
    "                    node_loss_dict[outp[k]] += loss[i]\n",
    "                    node_freq_dict[outp[k]] += 1\n",
    "                    node_avg_loss_dict[outp[k]] += node_loss_dict[outp[k]]/node_freq_dict[outp[k]]\n",
    "    for i in range(len(inputs)):\n",
    "        inp = inputs[i].split(\",\")\n",
    "        outp = outputs[i].split(\",\")\n",
    "        for j in range(len(inp)):\n",
    "            for k in range(len(outp)):\n",
    "                graph.add_edge(inp[j], outp[k], weight = weight_dict2[(inp[j], outp[k])])\n",
    "    return graph, node_loss_dict, node_freq_dict, node_avg_loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e537876-1fbc-4c24-b05d-1ad0161fb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_scores_with_initialization(graph, initialization, max_iter=100):\n",
    "    return nx.pagerank(graph, weight = \"weight\", nstart = initialization)\n",
    "\n",
    "def get_eHITS_score(graph, max_in_degree, max_out_degree, node_loss_dict, node_initialization = False, n_iter = 20):\n",
    "    nodes = list(graph.nodes())\n",
    "    hubs = {node: 1 for node in list(graph.nodes())}\n",
    "    authorities = {node: 1 for node in list(graph.nodes())}\n",
    "    if node_initialization == True:\n",
    "        hubs = {node: node_loss_dict[node] for node in list(graph.nodes())}\n",
    "        authorities = {node: node_loss_dict[node] for node in list(graph.nodes())}\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        for node_i in nodes:\n",
    "            authorities[node_i] = sum(hubs[node_j]*graph[node_j][node_i]['weight']/max_in_degree for node_j in nodes if node_i in list(graph[node_j]))\n",
    "            hubs[node_i] = sum(authorities[node_j]*graph[node_i][node_j]['weight']/max_out_degree for node_j in list(graph[node_i]))\n",
    "        s = sum(hubs[node]**2 for node in nodes)\n",
    "        print(s)\n",
    "        sum_hubs = np.sqrt(s)\n",
    "        sum_authorities = np.sqrt(sum(authorities[node]**2 for node in nodes))\n",
    "        hubs = {node: hubs[node]/sum_hubs if sum_hubs > 0 else 0 for node in nodes}\n",
    "        authorities = {node: authorities[node]/sum_authorities if sum_authorities > 0 else 0 for node in nodes}\n",
    "    return hubs, authorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1e95c7d-0de3-444a-a5d4-d7f30a0e419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_n_scale_pagerank_to_with_90_quantile(pr):\n",
    "    max_pr = np.percentile(list(pr.values()), 90)\n",
    "    scores = {}\n",
    "    for key, value in pr.items():\n",
    "        scores[key] = round(value/max_pr*100,2)\n",
    "    return scores\n",
    "\n",
    "def normalize_n_scale_pagerank_to_with_90_quantile_list(l):\n",
    "    max_pr = np.percentile(l, 90)\n",
    "    scores = []\n",
    "    for value in l:\n",
    "        scores.append(round(value/max_pr*100,2))\n",
    "    return scores\n",
    "\n",
    "def normalize_n_scale_pagerank_to_with_90_quantile_list_n_whole(l, pr):\n",
    "    max_pr = np.percentile(list(pr.values()), 90)\n",
    "    scores = []\n",
    "    for value in l:\n",
    "        scores.append(round(value/max_pr*100,2))\n",
    "    return scores\n",
    "\n",
    "def normalize_n_scale_score(dictionary):\n",
    "    max_pr = np.max(list(dictionary.values()))\n",
    "    scores = {}\n",
    "    for key, value in dictionary.items():\n",
    "        scores[key] = round(value/max_pr*100,2)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b53bbb0e-d7be-4f40-86f4-9e14a3c47605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correlation(dict1, dict2):\n",
    "    l1 = list(dict1.values())\n",
    "    l2 = list(dict2.values())\n",
    "    correlation, p_value = spearmanr(l1, l2)\n",
    "    return correlation, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad1c779d-4a97-4f06-a1a8-53b0addafc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataframe(\"example_dataset.csv\") #read the dataset\n",
    "inputs, outputs, loss = data_cleaning_process(df, with_loss_amount = True) # Clean the data and get inputs, outputs and the corresponding lossAmount\n",
    "graph, node_loss_dict, node_freq_dict, node_avg_loss_dict = graph_construction_return_loss_n_frequency(inputs, outputs, loss) # Construct the Identity Ecosystem graph with each edge weight being the occurrence frequency\n",
    "graph2, node_loss_dict, node_freq_dict, node_avg_loss_dict = graph_construction_using_loss_weight_return_loss_n_frequency(inputs, \n",
    "                                                                                                                          outputs, loss) # Construct the Identity Ecosystem graph with each edge weight being the fiancial loss amount\n",
    "pr_w_loss_initial = get_pr_scores_with_initialization(graph, node_loss_dict, max_iter=100) # Get the PageRank scores of each node using the graph with each edge weight being the occurrence frequency\n",
    "pr_w_loss_initial_w_loss_weight = get_pr_scores_with_initialization(graph2, node_loss_dict, max_iter=100) # Get the PageRank scores of each node using the graph with each edge weight being the fiancial loss amount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a716ef7-59ad-422d-8d47-6b4ea00ba210",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_in_degree_freq = max(dict(graph.in_degree(weight = 'weight')).values())\n",
    "max_out_degree_freq = max(dict(graph.out_degree(weight = 'weight')).values())\n",
    "\n",
    "max_in_degree_loss = max(dict(graph2.in_degree(weight = 'weight')).values())\n",
    "max_out_degree_loss = max(dict(graph2.out_degree(weight = 'weight')).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28d5a113-9876-4881-a9dc-49f23b25ae44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 20/20 [00:00<00:00, 1739.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4411591220850477\n",
      "0.3499666696534291\n",
      "0.32429466461000733\n",
      "0.321704055909316\n",
      "0.32191071507847957\n",
      "0.3221847876001393\n",
      "0.32233426916783353\n",
      "0.32240516747824594\n",
      "0.3224373740549394\n",
      "0.32245176889618793\n",
      "0.3224581600041654\n",
      "0.322460989497171\n",
      "0.3224622406276936\n",
      "0.3224627935451053\n",
      "0.3224630378396689\n",
      "0.32246314576450735\n",
      "0.3224631934414869\n",
      "0.32246321450288024\n",
      "0.32246322380670484\n",
      "0.32246322791663246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hubs_w_freq, authorities_w_freq = get_eHITS_score(graph, max_in_degree_freq, max_out_degree_freq, node_loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ff26662-1e7a-459f-882c-f61ede60551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 20/20 [00:00<00:00, 1737.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5076085217033446\n",
      "0.341936196773205\n",
      "0.3127145789733697\n",
      "0.30214251424256616\n",
      "0.2976392871064416\n",
      "0.2955595287312502\n",
      "0.2945605184698211\n",
      "0.2940709233265159\n",
      "0.2938285556456776\n",
      "0.2937079698361247\n",
      "0.2936478235560877\n",
      "0.2936177858596671\n",
      "0.2936027752939929\n",
      "0.29359527179590617\n",
      "0.2935915203504673\n",
      "0.29358964463238746\n",
      "0.29358870673887577\n",
      "0.29358823776568366\n",
      "0.2935880032635691\n",
      "0.2935878860041769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hubs_w_loss, authorities_w_loss = get_eHITS_score(graph2, max_in_degree_loss, max_out_degree_loss, node_loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5519b389-e829-4a1e-9b8f-110f23f82763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 20/20 [00:00<00:00, 1734.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214416811286.41974\n",
      "0.3187335378011907\n",
      "0.32145680929664106\n",
      "0.32191435211850206\n",
      "0.3221940399676394\n",
      "0.3223390757167141\n",
      "0.32240738843480304\n",
      "0.3224383717635382\n",
      "0.3224522127224551\n",
      "0.32245835665625194\n",
      "0.32246107648247424\n",
      "0.3224622790754359\n",
      "0.32246281053356635\n",
      "0.32246304534508463\n",
      "0.322463149080151\n",
      "0.322463194906187\n",
      "0.32246321514991005\n",
      "0.32246322409252803\n",
      "0.32246322804289357\n",
      "0.3224632297879489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hubs_w_freq_initLoss, authorities_w_freq_initLoss = get_eHITS_score(graph, max_in_degree_freq, \n",
    "                                                                    max_out_degree_freq, node_loss_dict, node_initialization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91cadfd-0000-4ba6-b060-806d118b3e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
